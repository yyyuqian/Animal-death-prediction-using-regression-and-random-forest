{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework2\n",
    "\n",
    "Please upload to Brightspace a **.zip** archive containing your Jupyter Notebook with solutions and all data required to reproduce your solutions. \n",
    "\n",
    "Please also prepare a **requirements.txt** file which lists all the packages that you have used for your homework, one package per line. This will allow us to install all required packages.\n",
    "\n",
    "Please name your .zip archive using your full name and student id as follows - **Firstname_Lastname_12345678_COMP47350_Homework2.zip**. \n",
    "\n",
    "For your Notebook, please split the code and explanations into many little cells so it is easy to see and read the results of each step of your solution. Please remember to name your variables and methods with self-explanatory names. Please remember to write comments and where needed, justifications, for the decisions you make and code you write. Feel free to revisit *tips_to_keep_your_ipython_notebook_readable_and_easy_to_debug.html* provided on Brightspace.\n",
    "\n",
    "Your code and analysis is like a story that awaits to be read, make it a nice story please. Always start with an introduction about the problem and your understanding of the problem domain and the data analytics solution. Then  describe the steps you take and your findings from each step. Aim to keep the notebook clear and concise, with the key code and discussion.\n",
    "\n",
    "The accepted file formats for the homework are:\n",
    "    - .ipynb\n",
    "    - .zip\n",
    "    - .pdf\n",
    "    - .csv\n",
    "    \n",
    "Please keep the whole code in a single notebook. Usage of external tools/files is discouraged for portability reasons. Files in any other format but mentioned above can be used but will be ignored and not considered for the submission (including .doc, .rar, .7z, .pages, .xlsx, .tex etc.). \n",
    "Any image format is allowed to be used as far as the images appear embedded in your report (.ipynb or .pdf or .html).\n",
    "\n",
    " \n",
    "**Deadline: Sunday, 26 April, 2020, midnight.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "**This homework focuses on building and evaluating prediction models for a particular problem and dataset.**\n",
    "The problem and data come from an animal shelter concerned with animal welfare and reducing the risk of animal death. The shelter wants to use the data collected about their animals to build a data analytics solution for death risk prediction to help them in their planning towards improving the welfare of the animals they shelter. The shelter collects some data for each animal they take in (columns in the dataset with keyword \"intake\") and also records the status of each animal when it left the shelter (columns in the dataset with keyword \"outcome\").\n",
    "The target variable to predict is a binary variable called **binary_outcome**. For this variable, the value “1” indicates that after intake, the animal outcome was negative, e.g., \"death\", while the value \"0\" indicates that the animal outcome was a positive one, e.g., was adopted or returned to the owner.\n",
    "The dataset we work with is a sample of the data released by this shelter:\n",
    "https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238\n",
    "\n",
    "The goal in this homework is to work with the data to build and evaluate prediction models that capture the relationship between the descriptive features and the target feature **binary_outcome**. For this homework you are asked to use the same dataset allocated to you in Homework1 (you can use your cleaned/prepared CSV from Homework1 or start from the raw dataset, clean it according to concepts covered in the lectures/labs, then use it for training prediction models).\n",
    " \n",
    "There are 5 parts for this homework. Each part has an indicative maximum percentage given in brackets, e.g., part (1) has a maximum of 25% shown as [25]. The total that can be achieved is 100.\n",
    "\n",
    "\n",
    "(1). [25] **Data Understanding and Prep:** Exploring relationships between feature pairs and selecting/transforming promising features based on a given training set.\n",
    "    - (1.1) Randomly shuffle the rows of your dataset and split the dataset into two datasets: 70% training and 30% test. Keep the test set aside. \n",
    "    - (1.2) On the training set:\n",
    "        - Plot the correlations between all the continuous features. Discuss what you see in these plots.\n",
    "        - For each continuous feature, plot its interaction with the target feature (a plot for each pair of continuous feature and target feature). Discuss what you observe from these plots, e.g., which continuous features seem to be better at predicting the target feature? Choose a subset of continuous features you find promising (if any). Justify your choices.\n",
    "        - For each categorical feature, plot its pairwise interaction with the target feature. Discuss what knowledge you gain from these plots, e.g., which categorical features seem to be better at predicting the target feature? Choose a subset of categorical features you find promising (if any). Justify your choices.\n",
    "      \n",
    "    \n",
    "(2). [15] **Predictive Modeling:** Linear Regression.  \n",
    "    - (2.1) On the training set, train a linear regression model to predict the target feature, using only the descriptive features selected in exercise (1) above. \n",
    "    - (2.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates the input feature to the target feature).    \n",
    "    - (2.3) Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (2.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained on the training (70%) dataset. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    \n",
    "(3). [15] **Predictive Modeling:** Logistic Regression.  \n",
    "    - (3.1) On the training set, train a logistic regression model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (3.2) Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model).    \n",
    "    - (3.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (3.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings.\n",
    "    \n",
    "    \n",
    "(4). [20] **Predictive Modeling:** Random Forest.  \n",
    "    - (3.1) On the training set, train a random forest model to predict the target feature, using the descriptive features selected in exercise (1) above.   \n",
    "    - (3.2) Can you interpret the random forest model? Discuss any knowledge you can gain in regard of the working of this model.   \n",
    "    - (3.3) Print the predicted target feature value for the first 10 training examples. Print the predicted class for the first 10 examples. Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1) and discuss your findings so far.\n",
    "    - (3.4) Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained when using the training (70%) dataset for evaluation. Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated train/test (70/30) splits. Compare the cross-validation metrics to those obtained on the single train/test split and to the out-of-sample error and discuss your findings.\n",
    "    \n",
    "(5). [25] **Improving Predictive Models.**\n",
    "    - (4.1) Which model of the ones trained above performs better at predicting the target feature? Is it more accurate than a simple model that always predicts the majority class (i.e., if binary_outcome=0 is the majority class in your dataset, the simple model always predicts 0 for the target feature)? Justify your answers.\n",
    "    - (4.2) Summarise your understanding of the problem and of your predictive modeling results so far. Can you think of any new ideas to improve the best model so far (e.g., by using furher data prep such as: feature selection, feature re-scaling, creating new features, combining models, or using other knowledge)? Please show how your ideas actually work in practice, by training and evaluating your proposed models. Summarise your findings so far.\n",
    "    - (4.3) Evaluate your best model on the new test dataset provided with this homework (in file '28032020-AnimalWelfareRisk-binaryOutcome-recent-1k.csv'). Discuss your findings.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
